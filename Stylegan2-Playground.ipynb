{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6889ab8d-0a22-4bba-8dc0-d5b03d23c844"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import PIL.Image\n",
    "import PIL.ImageSequence\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "from IPython.display import display, clear_output\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "import math\n",
    "import glob\n",
    "import csv\n",
    "from functools import partial\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import colorsys\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import run_projector\n",
    "import projector\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't say 1.14.0, it probably will not work. Stylegan2 is picky.\n",
    "# Additionally, you'll need some compiler so nvcc can work.\n",
    "# In summary, the best time for google to stop making breaking changes to tensorflow was 2 years ago\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d32e7195-2958-409b-b272-7284e737ec20"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 0. Load network snapshots\n",
    "##\n",
    "\n",
    "# From https://mega.nz/#!PeIi2ayb!xoRtjTXyXuvgDxSsSMn-cOh-Zux9493zqdxwVMaAzp4 - gwern animefaces stylegan2\n",
    "input_sg_name = \"2020-01-11-skylion-stylegan2-animeportraits-networksnapshot-024664.pkl\"\n",
    "\n",
    "tflib.init_tf()\n",
    "\n",
    "# Load pre-trained network.\n",
    "with open(input_sg_name, 'rb') as f:\n",
    "    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.    \n",
    "    _G, _D, Gs = pickle.load(f)\n",
    "        \n",
    "# Print network details.\n",
    "Gs.print_layers()\n",
    "_D.print_layers()\n",
    "\n",
    "# For projection\n",
    "proj = projector.Projector()\n",
    "proj.set_network(Gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4648bf80-75ce-4dd1-beab-bf1b1080d3fc"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 1. Plain generation\n",
    "##\n",
    "\n",
    "# \"Ellipse around a point but probably a circle since it's 512 dimensions\" laten\n",
    "def circ_generator(latents_interpolate):\n",
    "    radius = 40.0\n",
    "\n",
    "    latents_axis_x = (latents_a - latents_b).flatten() / la.norm(latents_a - latents_b)\n",
    "    latents_axis_y = (latents_a - latents_c).flatten() / la.norm(latents_a - latents_c)\n",
    "\n",
    "    latents_x = math.sin(math.pi * 2.0 * latents_interpolate) * radius\n",
    "    latents_y = math.cos(math.pi * 2.0 * latents_interpolate) * radius\n",
    "\n",
    "    latents = latents_a + latents_x * latents_axis_x + latents_y * latents_axis_y\n",
    "    return latents\n",
    "\n",
    "# Generate images from a list of latents\n",
    "def generate_from_latents(latent_list, truncation_psi):\n",
    "    array_list = []\n",
    "    image_list = []\n",
    "    for latents in latent_list:\n",
    "        # Generate image.\n",
    "        fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "        images = Gs.run(latents, None, truncation_psi=truncation_psi, randomize_noise=False, output_transform=fmt)\n",
    "        array_list.append(images[0])\n",
    "        image_list.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
    "        \n",
    "    return array_list, image_list\n",
    "\n",
    "def mse(x, y):\n",
    "    return (np.square(x - y)).mean()\n",
    "\n",
    "# Generate from a latent generator, keeping MSE between frames constant\n",
    "def generate_from_generator_adaptive(gen_func):\n",
    "    max_step = 1.0\n",
    "    current_pos = 0.0\n",
    "    \n",
    "    change_min = 10.0\n",
    "    change_max = 11.0\n",
    "    \n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    \n",
    "    current_latent = gen_func(current_pos)\n",
    "    current_image = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "    array_list = []\n",
    "    \n",
    "    while(current_pos < 1.0):\n",
    "        array_list.append(current_image)\n",
    "        \n",
    "        lower = current_pos\n",
    "        upper = current_pos + max_step\n",
    "        current_pos = (upper + lower) / 2.0\n",
    "        \n",
    "        current_latent = gen_func(current_pos)\n",
    "        current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "        current_mse = mse(array_list[-1], current_image)\n",
    "        \n",
    "        while current_mse < change_min or current_mse > change_max:\n",
    "            if current_mse < change_min:\n",
    "                lower = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "            \n",
    "            if current_mse > change_max:\n",
    "                upper = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "                \n",
    "            \n",
    "            current_latent = gen_func(current_pos)\n",
    "            current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "            current_mse = mse(array_list[-1], current_image)\n",
    "        print(current_pos, current_mse)\n",
    "        \n",
    "    return array_list\n",
    "\n",
    "# We have to do truncation ourselves, since we're not using the combined network\n",
    "def truncate(dlatents, truncation_psi, maxlayer = 16):\n",
    "    dlatent_avg = tf.get_default_session().run(Gs.own_vars[\"dlatent_avg\"])\n",
    "    layer_idx = np.arange(16)[np.newaxis, :, np.newaxis]\n",
    "    ones = np.ones(layer_idx.shape, dtype=np.float32)\n",
    "    coefs = tf.where(layer_idx < maxlayer, truncation_psi * ones, ones)\n",
    "    return tf.get_default_session().run(tflib.lerp(dlatent_avg, dlatents, coefs))\n",
    "\n",
    "# Generate image with disentangled latents as input\n",
    "def generate_images_from_dlatents(dlatents, truncation_psi = 1.0, randomize_noise = True):\n",
    "    if not truncation_psi is None:\n",
    "        dlatents_trunc = truncate(dlatents, truncation_psi)\n",
    "    else:\n",
    "        dlatents_trunc = dlatents\n",
    "        \n",
    "    # Run the network\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    result_image = Gs.components.synthesis.run(\n",
    "        dlatents_trunc.reshape((-1, 16, 512)),\n",
    "        randomize_noise = randomize_noise,\n",
    "        minibatch_size = 1,\n",
    "        output_transform=fmt\n",
    "    )[0]\n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just build a single image, randomly\n",
    "arrays, images = generate_from_latents([np.random.randn(1, Gs.input_shape[1])], 0.5)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init randomizer, load latents\n",
    "rnd = np.random # Could seed here if you want to\n",
    "latents_a = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_b = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_c = rnd.randn(1, Gs.input_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"latents.npy\"):\n",
    "    latents_a, latents_b, latents_c = np.load(\"latents.npy\")\n",
    "np.save(\"latents.npy\", np.array([latents_a, latents_b, latents_c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed input example, with dlatents as intermediate\n",
    "dlatents = Gs.components.mapping.run(latents_a, None)[0]\n",
    "PIL.Image.fromarray(generate_images_from_dlatents(dlatents, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8073d854-c890-4238-a40e-c2891d071785"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Circular interpolation example\n",
    "\"\"\"\n",
    "array_list = generate_from_generator_adaptive(circ_generator)\n",
    "clip = moviepy.editor.ImageSequenceClip(array_list, fps=60)\n",
    "clip.ipython_display()\n",
    "clip.write_videofile(\"out.mp4\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip.write_videofile(\"out.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# 3. Projection - encoding images into latent space\n",
    "##\n",
    "\n",
    "# Projects an image into dlatent space and returns the dlatents\n",
    "def encode_image(image, steps=1000, verbose=True):\n",
    "    image_processed = np.array(copy.deepcopy(image).convert('RGB').resize((512, 512), resample = PIL.Image.LANCZOS)) / 255.0\n",
    "    image_processed = (image_processed.transpose(2, 0, 1) - 0.5) * 2.0\n",
    "    image_processed = np.array([image_processed])\n",
    "    proj.num_steps = steps\n",
    "    proj.start(image_processed)\n",
    "    while proj.get_cur_step() < steps:\n",
    "        if verbose:\n",
    "            print('\\rProjection: Step %d / %d ... ' % (proj.get_cur_step(), steps), end='', flush=True)\n",
    "        proj.step()\n",
    "    print('\\r', end='', flush=True)\n",
    "    return proj.get_dlatents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that projection has a random component - if you're not happy with the result, probably retry a few times\n",
    "# For best results, probably have a single person facing the camera with a neutral white background\n",
    "image = PIL.Image.open(\"input.png\") # Default is toshiko koshijima\n",
    "proj_dlatents = encode_image(image, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_projected = generate_images_from_dlatents(proj_dlatents, 0.4)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.array(image))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f7fb3de5-155a-4b90-86b6-ceb8f83dabeb"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 3. Modification - prepararing data for training dlatent dirs\n",
    "##\n",
    "\n",
    "# Generate samples to pass to the tagger model\n",
    "def generate_one_for_latentdirs(truncation_psi = 0.7, randomize_noise = True):\n",
    "    latents = rnd.randn(1, Gs.input_shape[1])\n",
    "    dlatents = Gs.components.mapping.run(latents, None)[0]\n",
    "    dlatents_trunc = truncate(dlatents, truncation_psi)\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    result_image = Gs.components.synthesis.run(\n",
    "        dlatents_trunc.reshape((-1, 16, 512)),\n",
    "        randomize_noise = randomize_noise,\n",
    "        minibatch_size = 1,\n",
    "        output_transform=fmt\n",
    "    )[0]\n",
    "    return latents, dlatents, result_image\n",
    "\n",
    "# This gets slower as we go so lets go in steps of 100 to see if that alleviates the issue\n",
    "# of course, this doesn't work, on account of my GPU memory being too small. minibatch 10, maybe?\n",
    "# I want a ti 2080 11gb quite badly\n",
    "def generate_many_for_latentdirs(truncation_psi = 0.7, randomize_noise = True):\n",
    "    latents = rnd.randn(100, Gs.input_shape[1])\n",
    "    dlatents = Gs.components.mapping.run(latents, None)\n",
    "    dlatents_trunc = truncate(dlatents, truncation_psi)\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    result_image = Gs.components.synthesis.run(\n",
    "        dlatents_trunc.reshape((-1, 16, 512)),\n",
    "        randomize_noise = randomize_noise,\n",
    "        minibatch_size = 10,\n",
    "        output_transform=fmt\n",
    "    )\n",
    "    return latents, dlatents, result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unfortunately, stylegan 2 only runs on tf 1.14, while deepdanbooru current uses tf 2 - so we just save images here\n",
    "# and tag them later (see DeepDanbooru-Playground notebook)\n",
    "\"\"\"\n",
    "latent_list = []\n",
    "dlatent_list = []\n",
    "image_list = []\n",
    "image_count = 104000 # Increment to resume\n",
    "while True:\n",
    "    tags_exist = set(map(lambda x: x.split(\"_\")[-1], glob.glob(\"image_tags_*\")))\n",
    "    tags_required = set(map(lambda x: x.split(\"_\")[-1], glob.glob(\"dlatents_for_tagging_*\")))\n",
    "    tags_todo = sorted(list(tags_required.difference(tags_exist)))\n",
    "    \n",
    "    if len(tags_todo) > 0:\n",
    "        print(\"Waiting for tagger, resuming in 120s, queue depth:\", len(tags_todo))\n",
    "        time.sleep(120)\n",
    "    else:\n",
    "        # Vary psi a little\n",
    "        psi = 0.7\n",
    "        if len(image_list) < 200:\n",
    "            psi = 0.5\n",
    "        if len(image_list) < 500:\n",
    "            psi = 0.6\n",
    "\n",
    "        latents, dlatents, image = generate_many_for_latentdirs(truncation_psi = psi)\n",
    "        latent_list.extend(latents)\n",
    "        dlatent_list.extend(dlatents)\n",
    "        image_list.extend(image)\n",
    "        #if len(image_list) % 100 == 0:\n",
    "        print(\".\")\n",
    "\n",
    "        image_count += 100\n",
    "        if image_count % 1000 == 0:\n",
    "            with open(\"dlatents_for_tagging_{}.pkl\".format(image_count), 'wb') as f:\n",
    "                pickle.dump((latent_list, dlatent_list, image_list), f)\n",
    "            print(\"Wrote\", \"dlatents_for_tagging_{}.pkl\".format(image_count))\n",
    "            latent_list = []\n",
    "            dlatent_list = []\n",
    "            image_list = []\n",
    "\"\"\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tags from the DeepDanbooru notebook, and probably re-load the latents, as we just restarted the kernel\n",
    "\"\"\"\n",
    "image_tags = []\n",
    "latent_list = []\n",
    "dlatent_list = []\n",
    "\n",
    "tags_exist = set(map(lambda x: x.split(\"_\")[-1], glob.glob(\"image_tags_*\")))\n",
    "for i in tags_exist:\n",
    "    with open(\"image_tags_\" + i, 'rb') as f:\n",
    "        image_tags_tmp = pickle.load(f)\n",
    "        \n",
    "    with open(\"dlatents_for_tagging_\" + i, 'rb') as f:    \n",
    "        latent_list_tmp, dlatent_list_tmp, _ = pickle.load(f)\n",
    "        \n",
    "    image_tags.extend(image_tags_tmp)\n",
    "    latent_list.extend(latent_list_tmp)\n",
    "    dlatent_list.extend(list(np.array(dlatent_list_tmp).reshape(-1, 16, 512)[:, 0, :]))\n",
    "    \n",
    "gc.collect()\n",
    "dlatents_for_regression = np.array(dlatent_list).reshape(-1, 512)    \n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into features for learning directions\n",
    "\"\"\"\n",
    "all_tags = collections.defaultdict(int)\n",
    "for tags in image_tags:\n",
    "    for tag in tags:\n",
    "        all_tags[tag] += 1\n",
    "tags_by_popularity = sorted(all_tags.items(), key = lambda x: x[1], reverse = True)\n",
    "eye_tags = list(filter(lambda x: x[0].endswith(\"_eyes\"), tags_by_popularity))\n",
    "hair_tags = list(filter(lambda x: x[0].endswith(\"_hair\"), tags_by_popularity))\n",
    "\n",
    "tag_binary_feats = {}\n",
    "tag_continuous_feats = {}\n",
    "for tag, _ in tags_by_popularity:\n",
    "    this_tag_feats = []\n",
    "    this_tag_feats_cont = []\n",
    "    for tag_list_for_dl in image_tags:\n",
    "        this_dl_tag_value = 0.0\n",
    "        this_dl_tag_value_cont = 0.0\n",
    "        if tag in tag_list_for_dl:\n",
    "            this_dl_tag_value = 1.0\n",
    "            this_dl_tag_value_cont = tag_list_for_dl[tag]\n",
    "        this_tag_feats.append(this_dl_tag_value)\n",
    "        this_tag_feats_cont.append(this_dl_tag_value_cont)\n",
    "    tag_binary_feats[tag] = np.array(this_tag_feats)\n",
    "    tag_continuous_feats[tag] = np.array(this_tag_feats_cont)\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn directions for tags (binarized) using logistic regression classification\n",
    "# Thought: Why not apply L1 regu here to enforce sparsity?\n",
    "# -> I feel this has neither improved nor worsened the situation (default settings)\n",
    "# -> With really extreme sparsity constraint, it does, not work very well. Is my assumption that dlatents should be\n",
    "#    sparse wrt tags wrong? Probably is.\n",
    "# -> For now, plain logistic regression gives best results\n",
    "def find_direction_binary(dlatents, targets):\n",
    "    #clf = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1000, C=0.001).fit(dlatents, targets)\n",
    "    clf = LogisticRegression().fit(dlatents, targets)\n",
    "    return np.repeat(clf.coef_.reshape(1,512), 16, axis = 0)\n",
    "\n",
    "# Alternate version: Learn directions by fitting a linear regression onto the probabilities\n",
    "# Questions:\n",
    "# * Problem should be sparse - Lasso / Elasticnet?\n",
    "#    -> ElasticNet does empathically Not work, regularizes everything to 0 (lol)\n",
    "#    -> Same for Lasso :(\n",
    "#    -> Lower alpha term? omit intercept?\n",
    "#    -> seems like it would be really useful, though (not to mention faster), since we _expect_ sparsity here.\n",
    "#    -> retry later with more data?\n",
    "#    -> oh lol I'm fucking dumb why was I doing this with the inputs for every level. they're identical. there's just 512.\n",
    "# * Not the most well-posed problem just generally -> really do need more samples. sighs.\n",
    "# * Conclusion currently: Doesn't work all that well even though in theory it totally should\n",
    "# * Or may it it would if you did not Fuck It Up you turbo moron. Retrying with fixed labels now.\n",
    "def find_direction_continuous(dlatents, targets):\n",
    "    clf = Lasso(alpha=0.01, fit_intercept=False).fit(dlatents, targets)\n",
    "    if np.abs(np.sum(clf.coef_)) == 0.0:\n",
    "        clf = Lasso(alpha=0.001, fit_intercept=False).fit(dlatents, targets)\n",
    "    return  np.repeat(clf.coef_.reshape(1,512), 16, axis = 0)\n",
    "\n",
    "# Alternate idea: Fit gauss mixture, gradient descend in that\n",
    "# -> Ooooor, possibly: gauss mixture mapping kind of simultaneous likelihood maximization type stuff?\n",
    "# -> that sounds super fun lets try it\n",
    "# -> But first, the Super Baby Version: Just move towards weighted mean\n",
    "# -> Works okish, but only okayish\n",
    "def find_distrib_continuous(dlatents, targets):\n",
    "    tag_mean = np.sum(dlatents * targets.reshape(-1, 1), axis = 0) / np.sum(targets)\n",
    "    return  np.repeat(tag_mean.reshape(1,512), 16, axis = 0)\n",
    "\n",
    "\"\"\"\n",
    "popular_tags = list(filter(lambda x: x[1] > 5000 and x[1] < 95000, tags_by_popularity))\n",
    "good_tags = popular_tags\n",
    "\"\"\"\n",
    "\n",
    "# Would still like to try to use the tagger network to do direct gradient descent of dlatents with tag loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make the directions a bit more orthogonal\n",
    "# In retrospect, this obviously works against stylegans own attempts to do the same thing, so e h\n",
    "# -> it did not work very well\n",
    "\"\"\"\n",
    "dlatent_pca = PCA()\n",
    "dlatent_pca.fit(dlatents_for_regression)\n",
    "dlatents_for_regression_transformed = dlatent_pca.transform(dlatents_for_regression)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "caf43dba-3f58-4444-be8a-ed3c0a947f18"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate actual directions for modifications\n",
    "\"\"\"\n",
    "tag_directions = {}\n",
    "for i, (tag, _) in enumerate(good_tags):\n",
    "    print(\"Estimating direction for\", tag, \"(\", i, \")\")\n",
    "    # Variant A: Binary labels, logistic regression\n",
    "    #tag_directions[tag] = find_direction_binary(dlatents_for_regression, tag_binary_feats[tag])\n",
    "    \n",
    "    # Variant B: Continuous labels (confidence from deepdanbooru), Lasso regression\n",
    "    tag_directions[tag] = find_direction_continuous(dlatents_for_regression, tag_continuous_feats[tag])\n",
    "    \n",
    "    # Variant C: means and move to mean\n",
    "    #tag_directions[tag] = find_distrib_continuous(dlatents_for_regression, tag_continuous_feats[tag])\n",
    "with open(\"tag_dir_cont.pkl\", 'wb') as f:\n",
    "    pickle.dump(tag_directions, f)\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# 4. Modification - basic\n",
    "##\n",
    "\n",
    "with open(\"tag_dir_cont.pkl\", 'rb') as f:\n",
    "    tag_directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the weights.\n",
    "# Sparse weights <=> most stems \n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.stem(tag_directions[\"face\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible idea: use only the top n (absolute value) weights to make the result sparser yet, top 1 in the extreme\n",
    "# case (i.e. use ONLY one dimension)\n",
    "# -> Does help disentangle some things (hairstyle and colour) but makes other things (hair colour and eye colour) worse\n",
    "# -> Not really an improvement\n",
    "\"\"\"\n",
    "top_n = 10\n",
    "for tag in tag_directions:\n",
    "    top_list = np.argsort(np.abs(tag_directions[tag][0,:]))\n",
    "    for i in range(16):\n",
    "        tag_directions[tag][i,top_list[:-top_n]] = 0.0\n",
    "\"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4775c770-0ac4-49f7-a9d2-55a56bfecc96"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Do some modification and display on a grid (move-to-mean ver.)\n",
    "dlatents_gen = Gs.components.mapping.run(latents_a, None)[0]\n",
    "\n",
    "im = PIL.Image.new('RGB', (128 * 5, 128 * 5))\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        factor_hair = (i / 5.0) \n",
    "        factor_mouth = (j / 5.0)\n",
    "    \n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        #dlatents_mod = dlatent_pca.transform(dlatents_mod.reshape(1, 16*512)).reshape(16, 512)\n",
    "        dlatents_mod = (1.0 - factor_hair) * dlatents_mod + tag_directions[\"black_hair\"] * factor_hair\n",
    "        dlatents_mod = (1.0 - factor_mouth) * dlatents_mod + tag_directions[\":d\"] * factor_mouth\n",
    "        #dlatents_mod = dlatent_pca.inverse_transform(dlatents_mod.reshape(1, 16*512)).reshape(16, 512)\n",
    "        \n",
    "        dlatents_mod_image = generate_images_from_dlatents(dlatents_mod, 0.7)\n",
    "        im.paste(PIL.Image.fromarray(dlatents_mod_image, 'RGB').resize((128, 128), resample = PIL.Image.LANCZOS), (128 * i, 128 * j))\n",
    "im \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45531cb8-0ee4-4af1-a7ab-354fa0083c1e"
    }
   },
   "outputs": [],
   "source": [
    "# Do some modification and display on a grid\n",
    "dlatents_gen = Gs.components.mapping.run(latents_a, None)[0]\n",
    "\n",
    "im = PIL.Image.new('RGB', (128 * 5, 128 * 5))\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        factor_hair = (i / 5.0) * 25.0\n",
    "        factor_mouth = (j / 5.0) * 25.0\n",
    "    \n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        #dlatents_mod = dlatent_pca.transform(dlatents_mod.reshape(1, 16*512)).reshape(16, 512)\n",
    "        dlatents_mod += -tag_directions[\"blonde_hair\"] / np.linalg.norm(tag_directions[\"blonde_hair\"]) * factor_hair \\\n",
    "                        +tag_directions[\"black_hair\"] / np.linalg.norm(tag_directions[\"black_hair\"]) * factor_hair\n",
    "        dlatents_mod += -tag_directions[\"closed_mouth\"] / np.linalg.norm(tag_directions[\"closed_mouth\"]) * factor_mouth \\\n",
    "                        +tag_directions[\"open_mouth\"] / np.linalg.norm(tag_directions[\"open_mouth\"]) * factor_mouth\n",
    "        #dlatents_mod = dlatent_pca.inverse_transform(dlatents_mod.reshape(1, 16*512)).reshape(16, 512)\n",
    "        \n",
    "        dlatents_mod_image = generate_images_from_dlatents(dlatents_mod, 0.7)\n",
    "        im.paste(PIL.Image.fromarray(dlatents_mod_image, 'RGB').resize((128, 128), resample = PIL.Image.LANCZOS), (128 * i, 128 * j))\n",
    "im    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some tags that are not all that helpful\n",
    "tags_use = list(tag_directions.keys())\n",
    "\n",
    "tags_use.remove(\"face\")\n",
    "tags_use.remove(\"portrait\")\n",
    "tags_use.remove(\"pillarboxed\")\n",
    "tags_use.remove(\"letterboxed\")\n",
    "tags_use.remove(\"frame\")\n",
    "tags_use.remove(\"border\")\n",
    "tags_use.remove(\"black_border\")\n",
    "tags_use.remove(\"close-up\")\n",
    "tags_use.remove(\"artist_name\")\n",
    "\n",
    "with open(\"tags_use.pkl\", 'wb') as f:\n",
    "    pickle.dump(tags_use, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "68ca55b8-efe3-49c7-8e64-62f1923d93ea"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 4. Modification - interactive\n",
    "##\n",
    "hair_eyes_only = False\n",
    "    \n",
    "tag_len = {}\n",
    "for tag in tag_directions:\n",
    "    tag_len[tag] = np.linalg.norm(tag_directions[tag].flatten())\n",
    "\n",
    "mod_latents = copy.deepcopy(latents_a)\n",
    "dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "def modify_and_sample(psi_pre, psi_post, truncate_pre, truncate_post, **kwargs):\n",
    "    if truncate_pre == True:\n",
    "        dlatents_mod = truncate(copy.deepcopy(dlatents_gen), psi_pre)\n",
    "    else:\n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        \n",
    "    for tag in kwargs:\n",
    "        dlatents_mod += tag_directions[tag] * kwargs[tag]  / tag_len[tag] * 25.0\n",
    "    value_widgets[\"psi_pre\"].value = str(round(psi_pre, 2))\n",
    "    value_widgets[\"psi_post\"].value = str(round(psi_post, 2))\n",
    "    \n",
    "    for tag in kwargs:\n",
    "        tag_value = round((np.dot(dlatents_mod.flatten(), tag_directions[tag].flatten()) / tag_len[tag]), 2)\n",
    "        value_widgets[tag].value = str(kwargs[tag]) + \" | \" + str(tag_value)\n",
    "    \n",
    "    display_psi = None\n",
    "    if truncate_post == True:\n",
    "        display_psi = psi_post\n",
    "    display(PIL.Image.fromarray(generate_images_from_dlatents(dlatents_mod, truncation_psi = display_psi), 'RGB'))\n",
    "\n",
    "psi_slider = widgets.FloatSlider(min = 0.0, max = 1.0, step = 0.01, value = 0.75, continuous_update = False, readout = False)\n",
    "psi_slider_post = widgets.FloatSlider(min = 0.0, max = 1.0, step = 0.01, value = 0.65, continuous_update = False, readout = False)\n",
    "if hair_eyes_only:\n",
    "    modify_tags = [tag for tag in tag_directions if \"_hair\" in tag or \"_eyes\" in tag or \"_mouth\" in tag]\n",
    "else:\n",
    "    with open(\"tags_use.pkl\", \"rb\") as f:\n",
    "        modify_tags = pickle.load(f)\n",
    "\n",
    "tag_widgets = {}\n",
    "for tag in modify_tags:\n",
    "    tag_widgets[tag] = widgets.FloatSlider(min = -5.0, max = 5.0, step = 0.005, continuous_update = False, readout = False)\n",
    "all_widgets = []\n",
    "\n",
    "sorted_widgets = sorted(tag_widgets.items(), key = lambda x: x[0])\n",
    "sorted_widgets = [(\"psi_pre\", psi_slider), (\"psi_post\", psi_slider_post)] + sorted_widgets\n",
    "value_widgets = {}\n",
    "for widget in sorted_widgets:\n",
    "    label_widget = widgets.Label(widget[0])\n",
    "    label_widget.layout.width = \"170px\"\n",
    "    \n",
    "    value_widget = widgets.Label(\"0.0+100.0\")\n",
    "    value_widget.layout.width = \"150px\"\n",
    "    value_widgets[widget[0]] = value_widget\n",
    "    \n",
    "    tag_hbox = widgets.HBox([label_widget, widget[1], value_widget])\n",
    "    tag_hbox.layout.width = \"320px\"\n",
    "    \n",
    "    all_widgets.append(tag_hbox)\n",
    "\n",
    "refresh = widgets.Button(description=\"New Sample\")\n",
    "modify = widgets.Button(description=\"Mutate\")\n",
    "reset_sliders = widgets.Button(description=\"Reset\")\n",
    "dlatent_encode = widgets.Button(description=\"Project input.png\")\n",
    "\n",
    "def new_sample(b):\n",
    "    global mod_latents\n",
    "    global dlatents_gen\n",
    "    mod_latents = np.random.randn(1, Gs.input_shape[1])\n",
    "    dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "    if psi_slider.value != 1.0:\n",
    "        psi_slider.value += 0.00000000001\n",
    "    else:\n",
    "        psi_slider.value -= 0.00000000001\n",
    "    \n",
    "def mutate(b):\n",
    "    global dlatents_gen\n",
    "    mod_dlatents_add = np.random.randn(16, 512)\n",
    "    dlatents_gen += mod_dlatents_add * 0.1\n",
    "    if psi_slider.value != 1.0:\n",
    "        psi_slider.value += 0.00000000001\n",
    "    else:\n",
    "        psi_slider.value -= 0.00000000001\n",
    "\n",
    "def reset(b):\n",
    "    for widget in tag_widgets:\n",
    "        if not widget in [\"truncate_pre\", \"truncate_post\", \"psi_pre\", \"psi_post\"]:\n",
    "            tag_widgets[widget].value = 0.0\n",
    "\n",
    "def encode(b):\n",
    "    global dlatents_gen\n",
    "    image = PIL.Image.open(\"input.png\")\n",
    "    dlatents_gen = encode_image(image, encode_iters.value)\n",
    "    if psi_slider.value != 1.0:\n",
    "        psi_slider.value += 0.00000000001\n",
    "    else:\n",
    "        psi_slider.value -= 0.00000000001\n",
    "            \n",
    "truncate_pre = widgets.ToggleButton(value=False, description='Truncate Pre')\n",
    "truncate_post = widgets.ToggleButton(value=True, description='Truncate Post')\n",
    "refresh.on_click(new_sample)\n",
    "modify.on_click(mutate)\n",
    "reset_sliders.on_click(reset)\n",
    "dlatent_encode.on_click(encode)\n",
    "\n",
    "encode_iters = widgets.IntSlider(min = 10, max = 2000, step = 1, value=200, continuous_update = False, readout = True)\n",
    "\n",
    "ui = widgets.Box(all_widgets + [refresh, modify, reset_sliders, truncate_pre, truncate_post, dlatent_encode, widgets.Label(\"Iterations\"), encode_iters])\n",
    "tag_widgets[\"psi_pre\"] = psi_slider\n",
    "tag_widgets[\"psi_post\"] = psi_slider_post\n",
    "\n",
    "ui.layout.flex_flow = 'row wrap'\n",
    "ui.layout.display = 'inline-flex'\n",
    "tag_widgets[\"truncate_pre\"] = truncate_pre\n",
    "tag_widgets[\"truncate_post\"] = truncate_post\n",
    "\n",
    "out = widgets.interactive_output(modify_and_sample, tag_widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "display(ui, out)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "stylegan2",
   "language": "python",
   "name": "stylegan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbpresent": {
   "slides": {
    "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655": {
     "id": "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655",
     "prev": null,
     "regions": {
      "234da276-ee07-47ef-b598-c4ba844db0b8": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": -0.0008051529790660225,
        "y": 0
       },
       "content": {
        "cell": "68ca55b8-efe3-49c7-8e64-62f1923d93ea",
        "part": "outputs"
       },
       "id": "234da276-ee07-47ef-b598-c4ba844db0b8",
       "x": 0
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
